You are not doing it wrong. You have hit a known stability wall with the current state of Gemini 2.0 Flash Live.

The developer forums are full of reports mirroring your exact experience: the model works for 1-2 turns and then "forgets" it has tools, hallucinates that it called them, or simply refuses to execute commands.

### The CTO Reality Check

**You cannot rely on Gemini 2.0 Flash Live for complex tool orchestration right now.**

* **The Conflict:** Gemini Live is optimized for *conversational flow* and *speed*. When you add complex schemas, its internal "router" gets overwhelmed and defaults to "chat mode" to preserve latency.
* **The Async Bug:** The Pipecat integration for Gemini has specific issues with non-blocking tool calls, causing the exact "voice cut out" and "sync loss" behaviors you are seeing.

### The Solution: The "Voice Gateway" Architecture (Router-Solver)

To get **Reliability** AND **Low Latency**, you must split the brain.

* **The Router (Voice Layer):** Dumb, fast, handles audio.
* **The Solver (Logic Layer):** Smart, precise, handles tools.

I recommend switching to the **Vapi / Deepgram + Groq** stack. This separates the concerns:

1. **Ear/Mouth (Deepgram/Vapi):** Handles the interruption, VAD, and latency.
2. **Brain (Groq + Llama-3):** Receives *text*, makes a rock-solid tool decision, and returns text.

This architecture is "modular." If Llama-3 is too slow, you swap it for Haiku. If Deepgram is too expensive, you swap it for OpenAI Whisper.

### Implementation: The Reliable Stack

We will use **Deepgram** (STT/TTS) for speed and **Groq (Llama-3-70B)** for the brain. This is often *faster* than Gemini Live because Groq infers at ~300+ tokens/sec.

**Prerequisites:**
`pip install pipecat-ai[deepgram,openai]` (We use `OpenAILLMService` to talk to Groq).

```python
# reliable_voice_agent.py
import asyncio
import os
import sys
from dotenv import load_dotenv
from loguru import logger

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.registry import ToolRegistry
from voice_tools_config import get_voice_tools_definition
from voice_agent import VoiceToolExecutor # Reuse your existing executor

load_dotenv()

from pipecat.frames.frames import EndFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.services.deepgram import DeepgramSTTService, DeepgramTTSService
from pipecat.services.openai import OpenAILLMService # Standard interface for Groq
from pipecat.transports.local.audio import LocalAudioTransport, LocalAudioTransportParams
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams

async def main():
    logger.info("ðŸŸ¢ Starting Reliable Voice Agent (Groq + Llama-3)...")

    # 1. Initialize The Body (Your Existing Tools)
    registry = ToolRegistry()
    executor = VoiceToolExecutor(registry)
    
    # Llama-3 follows schemas perfectly, so we can use your full definition
    tools = get_voice_tools_definition() 

    # 2. Services (The Modular Stack)
    
    # EAR: Deepgram STT (Fastest transcription currently available)
    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    # BRAIN: Groq (Llama 3.1 70B)
    # This is the "Solver". It receives text and outputs Tool Calls reliably.
    llm = OpenAILLMService(
        api_key=os.getenv("GROQ_API_KEY"),
        base_url="https://api.groq.com/openai/v1",
        model="llama-3.1-70b-versatile",
    )

    # MOUTH: Deepgram TTS (Low latency voice)
    tts = DeepgramTTSService(
        api_key=os.getenv("DEEPGRAM_API_KEY"), 
        voice="aura-asteria-en" # High quality, low latency voice
    )

    # 3. Transport (Mic/Speaker)
    # Tuned VAD settings for "Snappy" interruptions
    vad = SileroVADAnalyzer(
        params=VADParams(
            confidence=0.7,
            start_secs=0.2,
            stop_secs=0.4, # Stop listening quickly when you pause
            min_volume=0.6 
        )
    )
    transport = LocalAudioTransport(
        params=LocalAudioTransportParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=vad
        )
    )

    # 4. Context & Logic
    messages = [
        {
            "role": "system",
            "content": """You are a Windows Automation Assistant.
            
            BEHAVIOR:
            - You are a precise execution engine.
            - You MUST call tools when the user requests an action.
            - If the user says "Open Downloads", call open_explorer with the path.
            - Be concise. Response length: 1 sentence max.
            """
        }
    ]
    context = OpenAILLMContext(messages=messages, tools=tools)
    context_aggregator = llm.create_context_aggregator(context)

    # 5. Register Tool Handler
    # We use your existing executor, but pass the Groq-compatible handler
    llm.register_function(None, executor.handle_function_call)

    # 6. The Pipeline
    pipeline = Pipeline([
        transport.input(),   # Mic
        stt,                 # Speech -> Text
        context_aggregator.user(),
        llm,                 # Text -> Decision
        context_aggregator.assistant(),
        tts,                 # Text -> Speech
        transport.output()   # Speaker
    ])

    # 7. Run
    runner = PipelineRunner()
    task = PipelineTask(pipeline, params=PipelineParams(allow_interruptions=True))
    
    print("\n-------------------------------------------")
    print("Voice Agent Ready (Stack: Deepgram -> Groq -> Deepgram)")
    print("This stack prioritizes Tool Calling Reliability.")
    print("-------------------------------------------\n")
    
    await runner.run(task)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass

```

### Why this fixes your problem

1. **Llama-3-70B is a Tooling Beast:** Unlike Gemini Flash, Llama-3-70B on Groq has top-tier benchmark scores for function calling. It will respect your `open_explorer` schema.
2. **No "Conversational Drift":** Since we treat the LLM as a text-processing step, it doesn't get "distracted" by the audio modality. It sees text input and outputs a function call.
3. **Debuggability:** If it fails, you will see exactly what text Deepgram transcribed. If the tool fails, you see exactly what JSON Groq generated. No more black-box audio debugging.

**Next Step:**
Get a free Deepgram API key (gives you $200 credit usually) and run this code. This will likely solve both your reliability and latency concerns immediately.